{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e497ca65",
   "metadata": {},
   "source": [
    "##### Author: Weisi Chen\n",
    "##### Last update: 1 May 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Python libraries for this exercise\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33fcd2",
   "metadata": {},
   "source": [
    "### Function to read one AFR XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac291fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to read AFR news from a given XML file\n",
    "# selected_type is an optional option that generates a filtered table by the news type;\n",
    "# The value can be one of the following: \n",
    "#       'Domain Prestige', 'Companies and Markets', 'News',\n",
    "#       'Chanticleer', 'Perspective', 'Weekend Fin', 'Opinion',\n",
    "#       'Stock Tables', 'Smart Investor', 'Poster', 'World', 'Market Wrap',\n",
    "#       'Property', 'Features', 'Life & Leisure', 'Financial Services',\n",
    "#       'Review', 'Accounting', 'Marketing & Media', 'Education',\n",
    "#       'Saleroom', 'Computers', 'Supplement'\n",
    "\n",
    "def read_afr(xml_file, selected_type = \"\"):\n",
    "    xtree = et.parse(xml_file)\n",
    "    dates_all = []\n",
    "    news_texts_all = []\n",
    "    headlines_all = []\n",
    "    sections_all = []\n",
    "\n",
    "    for node in xtree.iter('TEXT'):\n",
    "        news_text = \"\"\n",
    "        for subnode in node.iter('p'):\n",
    "            whole = subnode.itertext()\n",
    "            for parts in whole:\n",
    "                news_text += parts\n",
    "        news_texts_all.append(news_text)\n",
    "\n",
    "    for node in xtree.iter('SECTION'):\n",
    "        sections_all.append(node.text)\n",
    "        \n",
    "    for node in xtree.iter('PUBLICATIONDATE'):\n",
    "        dates_all.append(node.text)\n",
    "        \n",
    "    # print(len(headlines_all), len(dates_all), len(news_texts_all))\n",
    "    news_df = pd.DataFrame(\n",
    "        {'date': dates_all,\n",
    "         # 'headline': headlines_all,\n",
    "         'text': news_texts_all,\n",
    "         'section': sections_all})\n",
    "    \n",
    "    if(selected_type):\n",
    "        print(\"Selected News Types: \", selected_type)\n",
    "        news_df = news_df.loc[news_df['section'] == selected_type]\n",
    "    \n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ed2d8",
   "metadata": {},
   "source": [
    "### Read all XML files within a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85b5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\AFR_20150101-20150131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150201-20150228.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150301-20150331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150401-20150430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150501-20150531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150601-20150630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150701-20150731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150801-20150831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20150901-20150930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20151001-20151031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20151101-20151130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20151201-20151231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160101-20160131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160201-20160229.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160301-20160331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160401-20160430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160501-20160531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160601-20160630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160701-20160731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160801-20160831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20160901-20160930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20161001-20161031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20161101-20161130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20161201-20161231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170101-20170131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170201-20170228.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170301-20170331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170401-20170430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170501-20170531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170601-20170630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170701-20170731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170801-20170831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20170901-20170930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20171001-20171031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20171101-20171130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20171201-20171231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180101-20180131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180201-20180228.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180301-20180331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180401-20180430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180501-20180531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180601-20180630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180701-20180731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180801-20180831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20180901-20180930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20181001-20181031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20181101-20181130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20181201-20181231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190101-20190131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190201-20190228.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190301-20190331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190401-20190430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190501-20190531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190601-20190630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190701-20190731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190801-20190831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20190901-20190930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20191001-20191031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20191101-20191130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20191201-20191231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200101-20200131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200201-20200229.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200301-20200331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200401-20200430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200501-20200531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200601-20200630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200701-20200731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200801-20200831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20200901-20200930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20201001-20201031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20201101-20201130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20201201-20201231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210101-20210131.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210201-20210228.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210301-20210331.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210401-20210430.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210501-20210531.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210601-20210630.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210701-20210731.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210801-20210831.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20210901-20210930.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20211001-20211031.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20211101-20211130.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "data\\AFR_20211201-20211231.xml\n",
      "Selected News Types:  Companies and Markets\n",
      "          date                                               text   \n",
      "3     20150131  BC Iron managing director Morgan Ball says he ...  \\\n",
      "4     20150131  Australia's biggest gold miner, Newcrest Minin...   \n",
      "5     20150131  Purchase gives Snowy a vital stake in the elec...   \n",
      "23    20150131  The Wiggins Island Coal Export Â­Terminal will ...   \n",
      "24    20150131  Seven Group Holdings has positioned itself to ...   \n",
      "...        ...                                                ...   \n",
      "2583  20211201  This content is produced by The Australian Fin...   \n",
      "2584  20211201  Andrew Forrest's LNG import venture in Port Ke...   \n",
      "2585  20211201  New York | Rising COVID-19 cases and the new o...   \n",
      "2586  20211201  Cambridge, Massachusetts | The chief executive...   \n",
      "2587  20211201  Jack Dorsey's simultaneous leadership of $US38...   \n",
      "\n",
      "                    section  \n",
      "3     Companies and Markets  \n",
      "4     Companies and Markets  \n",
      "5     Companies and Markets  \n",
      "23    Companies and Markets  \n",
      "24    Companies and Markets  \n",
      "...                     ...  \n",
      "2583  Companies and Markets  \n",
      "2584  Companies and Markets  \n",
      "2585  Companies and Markets  \n",
      "2586  Companies and Markets  \n",
      "2587  Companies and Markets  \n",
      "\n",
      "[38240 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filenames = sorted(glob.glob('data/AFR*.xml'))\n",
    "# filenames = filenames[0:1]\n",
    "docs = []\n",
    "flag = True\n",
    "df = pd.DataFrame()\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    news = read_afr(filename, \"Companies and Markets\")\n",
    "    if flag:\n",
    "        df = pd.DataFrame(news['date'],columns=(['date']))\n",
    "        df['text']=news['text']\n",
    "        df['section']=news['section']\n",
    "        flag = False\n",
    "    else:\n",
    "        df = pd.concat([df,news],axis=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23da3e",
   "metadata": {},
   "source": [
    "### Count of sentences and words in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a85f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence number: 29.124973849372385\n",
      "word number: 590.1399058577406\n"
     ]
    }
   ],
   "source": [
    "# average sentence and word length\n",
    "sum = 0\n",
    "for row in df['text']:\n",
    "    sum += row.count('.')\n",
    "print(\"sentence number:\",sum/len(df['text']))\n",
    "\n",
    "sum = 0\n",
    "for row in df['text']:\n",
    "    sum += row.count(' ')\n",
    "print(\"word number:\",sum/len(df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399f424",
   "metadata": {},
   "source": [
    "### Data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0958611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dbff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean all the text. \n",
    "FullData = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = ' '.join(lemma.lemmatize(word) for word in text.split())\n",
    "    return text\n",
    "df['clean_text'] = df['text'].apply(clean_text).str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0324fc5",
   "metadata": {},
   "source": [
    "### Executing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59898d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sklearn and functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "N_TOPIC = 100\n",
    "# Using the Count vectorizer\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40321db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1950.5870633125305 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lda_tf_model_100.jl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the LDA model, and then fit the model using TF or TFIDF.\n",
    "lda_tf = LatentDirichletAllocation(n_components=N_TOPIC, learning_method='online',\n",
    "                                   random_state=42, max_iter=10) \n",
    "\n",
    "# TF DTM\n",
    "lda_top_tf = lda_tf.fit_transform(dtm_tf)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# saving model\n",
    "import joblib\n",
    "model_filename = 'lda_tf_model_'.strip() + str(N_TOPIC).strip() + '.jl'.strip()\n",
    "joblib.dump(lda_tf, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_features=1000)\n",
    "# dtm_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "# TFIDF DTM\n",
    "# lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "# lda_top_tfidf = lda_tfidf.fit_transform(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055b3f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "lda_tf_20=joblib.load('lda_tf_model_20.jl')\n",
    "#lda_tf_50=joblib.load('lda_tf_model_50.jl')\n",
    "lda_tf_100=joblib.load('lda_tf_model_100.jl')\n",
    "#lda_tf_500=joblib.load('lda_tf_model_500.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b2b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the LDA results using TF (Count Vectorizer)\n",
    "print(lda_tf.components_)\n",
    "print(lda_tf.components_.shape)\n",
    "print('perplexity: ')\n",
    "print(lda_tf.perplexity(dtm_tf, sub_sampling=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the LDA results using TF-IDF\n",
    "# print(lda_tfidf.components_)\n",
    "# print(lda_tfidf.components_.shape)\n",
    "# print('perplexity: ')\n",
    "# print(lda_tfidf.perplexity(dtm_tfidf, sub_sampling=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e0d648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 2: \n",
      "Topic  0 :  0.011682243336815615 %\n",
      "Topic  1 :  0.7469126235595284 %\n",
      "Topic  2 :  0.011682243279743106 %\n",
      "Topic  3 :  0.011682243302025268 %\n",
      "Topic  4 :  27.65166032072448 %\n",
      "Topic  5 :  0.011682243124796736 %\n",
      "Topic  6 :  6.74410568424459 %\n",
      "Topic  7 :  12.119470659605742 %\n",
      "Topic  8 :  33.44822797360226 %\n",
      "Topic  9 :  0.011682243208406669 %\n",
      "Topic  10 :  0.011682243215414135 %\n",
      "Topic  11 :  14.275721848642492 %\n",
      "Topic  12 :  0.011682243163956564 %\n",
      "Topic  13 :  0.011682243205807468 %\n",
      "Topic  14 :  1.738998479924205 %\n",
      "Topic  15 :  0.011682243195455065 %\n",
      "Topic  16 :  1.8559061974896331 %\n",
      "Topic  17 :  1.2904915369364034 %\n",
      "Topic  18 :  0.01168224305176054 %\n",
      "Topic  19 :  0.011682243186493033 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 2: \")\n",
    "for i,topic in enumerate(lda_top_tf[2]):\n",
    "    print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4af0c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "say time way need make people like think going big \n",
      "\n",
      "Topic 1: \n",
      "adam soul sea suez brickwork sexual softbank flu california canada \n",
      "\n",
      "Topic 2: \n",
      "rate bank economy growth central economic inflation reserve fed policy \n",
      "\n",
      "Topic 3: \n",
      "wilson progressive foundation pearce exclude aberdeen beddow preston barker potter \n",
      "\n",
      "Topic 4: \n",
      "ubs autonomous lawcock decentralised magnetite sino citic grange mineralogy glyn \n",
      "\n",
      "Topic 5: \n",
      "crown casino star packer hotel gaming resort gambling sydney operator \n",
      "\n",
      "Topic 6: \n",
      "patts unanimously groundwater arizona delegation barlow sacred crouch spacex torres \n",
      "\n",
      "Topic 7: \n",
      "organic packaging plastic amcor farrell delia abotomey glove protective ansell \n",
      "\n",
      "Topic 8: \n",
      "woman csl female diversity recall hunt elder bramble men blood \n",
      "\n",
      "Topic 9: \n",
      "infection tomlinson hayes hospitalisation ballooning calvary socialist aws stockpiling taxonomy \n",
      "\n",
      "Topic 10: \n",
      "energy power solar electricity renewable generation plant agl storage battery \n",
      "\n",
      "Topic 11: \n",
      "wisetech pursuit memory adairs disability husband embarrassing hume sex tagliaferro \n",
      "\n",
      "Topic 12: \n",
      "debt credit capital balance cash sheet rating suisse king raising \n",
      "\n",
      "Topic 13: \n",
      "election president american america labour wage administration republican house washington \n",
      "\n",
      "Topic 14: \n",
      "robotics avocado debney alastair blueberry smartest trafigura cme brereton cbi \n",
      "\n",
      "Topic 15: \n",
      "health care healthcare downer prospectus spotless cook dixon sector pro \n",
      "\n",
      "Topic 16: \n",
      "ore iron tonne steel price fortescue production miner pilbara export \n",
      "\n",
      "Topic 17: \n",
      "esg cargo aplng bottleneck chairwoman shipped netback upthe gladstone duke \n",
      "\n",
      "Topic 18: \n",
      "oil search barrel png brookfield papua guinea total botten oneill \n",
      "\n",
      "Topic 19: \n",
      "customer digital service consumer platform competition online accc amazon product \n",
      "\n",
      "Topic 20: \n",
      "airline virgin qantas flight domestic international route air aviation joyce \n",
      "\n",
      "Topic 21: \n",
      "group share billion stake sale capital acquisition growth sell sold \n",
      "\n",
      "Topic 22: \n",
      "loan bank credit lending lender mortgage payment home finance customer \n",
      "\n",
      "Topic 23: \n",
      "aedt thorney tynan praemium hurricane hansen dismissing everincreasing foolish refrain \n",
      "\n",
      "Topic 24: \n",
      "private hospital black saudi ramsay medibank patient surgery public healthscope \n",
      "\n",
      "Topic 25: \n",
      "venture joint aluminium alumina smelter partner fletcher alcoa factory product \n",
      "\n",
      "Topic 26: \n",
      "infrastructure road toll group link billion transurban transport sydney traffic \n",
      "\n",
      "Topic 27: \n",
      "hong kong singapore merchant pacific later alliance china liberty international \n",
      "\n",
      "Topic 28: \n",
      "asia japan asian malaysia region indonesia southeast van thailand middle \n",
      "\n",
      "Topic 29: \n",
      "smith russell orica separation guy explosive calderon inclusive fitness oricas \n",
      "\n",
      "Topic 30: \n",
      "hill taylor cimic contractor construction roy incident group leighton holland \n",
      "\n",
      "Topic 31: \n",
      "britain party country people social power paris leader attack world \n",
      "\n",
      "Topic 32: \n",
      "rio tinto mining miner billion bhp jacques commodity glencore asset \n",
      "\n",
      "Topic 33: \n",
      "apple bitcoin point spot seng blockchain changeasx changegold cryptocurrency change \n",
      "\n",
      "Topic 34: \n",
      "unconventional slate emotion disconnect drillers spy scepter molecule fracturing ubiquitous \n",
      "\n",
      "Topic 35: \n",
      "farm water farmer grain crop drought moore pivot graincorp agricultural \n",
      "\n",
      "Topic 36: \n",
      "sale quarter month march december june september rose january period \n",
      "\n",
      "Topic 37: \n",
      "project contract plant billion construction cost work development expected expansion \n",
      "\n",
      "Topic 38: \n",
      "airbnb greencross lics sealed elia coincide bestknown wellington zhaopin milan \n",
      "\n",
      "Topic 39: \n",
      "risk bank regulator capital regulation financial regulatory change commission prudential \n",
      "\n",
      "Topic 40: \n",
      "government state industry federal nsw minister support national green decision \n",
      "\n",
      "Topic 41: \n",
      "pitt orocobre packed lithiumion perdis home brine referenced discharge highfrequency \n",
      "\n",
      "Topic 42: \n",
      "network telstra mobile nbn tpg customer telco service optus plan \n",
      "\n",
      "Topic 43: \n",
      "bond investor rate yield asset bank fund equity billion risk \n",
      "\n",
      "Topic 44: \n",
      "cain quadrant poole documentary devil ala childhood foodland aig gonzalez \n",
      "\n",
      "Topic 45: \n",
      "infectious coombe preindustrial panicked birketu channelled upend fabo contrition coral \n",
      "\n",
      "Topic 46: \n",
      "zip funeral nigel mcfarlane mcgrath pride orora happiness staffer twoweek \n",
      "\n",
      "Topic 47: \n",
      "court financial claim commission action legal case asic law report \n",
      "\n",
      "Topic 48: \n",
      "pandemic novartis influenza seqirus whisky nondisclosure carton mould projectsmr booth \n",
      "\n",
      "Topic 49: \n",
      "price supply demand cost production capacity industry sector higher producer \n",
      "\n",
      "Topic 50: \n",
      "rare management lynas point owner pricing cost offered pretty end \n",
      "\n",
      "Topic 51: \n",
      "deal billion merger agreement asset acquisition transaction talk potential offer \n",
      "\n",
      "Topic 52: \n",
      "davis healy cromwell quiksilver pierre deleted familiarity inman lewinns callaghan \n",
      "\n",
      "Topic 53: \n",
      "bain cochlear hearing tasmanian implant piper fish remark salmon private \n",
      "\n",
      "Topic 54: \n",
      "santos pipeline gas cooper apa williams narrabri central coates mccormack \n",
      "\n",
      "Topic 55: \n",
      "woolworth food cole supermarket sale supplier price chain product retailer \n",
      "\n",
      "Topic 56: \n",
      "fuel caltex refinery petrol arrium whyalla refining viva conversion ampol \n",
      "\n",
      "Topic 57: \n",
      "bhp bhps mackenzie billion shale henry exploration production scarborough asset \n",
      "\n",
      "Topic 58: \n",
      "coal thermal tonne queensland coking miner hunter mining export whitehaven \n",
      "\n",
      "Topic 59: \n",
      "store retailer retail sale online brand myer chain customer jones \n",
      "\n",
      "Topic 60: \n",
      "bank banking customer westpac anz nab cba commonwealth royal banker \n",
      "\n",
      "Topic 61: \n",
      "south plan week test march result decision material month called \n",
      "\n",
      "Topic 62: \n",
      "technology data product tech software startup vaccine medical innovation help \n",
      "\n",
      "Topic 63: \n",
      "shareholder share board offer investor bid takeover vote meeting director \n",
      "\n",
      "Topic 64: \n",
      "medium news corp fairfax gordon fox network regional cross murdoch \n",
      "\n",
      "Topic 65: \n",
      "insurance car vehicle lithium electric insurer life tesla battery industry \n",
      "\n",
      "Topic 66: \n",
      "milk murray formula dairy product infant farmer baby brand pet \n",
      "\n",
      "Topic 67: \n",
      "emission climate carbon change target net global policy reduce world \n",
      "\n",
      "Topic 68: \n",
      "ferrari affidavit chen julie nasdaqlisted von munger lit defect glitch \n",
      "\n",
      "Topic 69: \n",
      "conveyancing inplay bight dahlsen ritchie dealbut fifield nominating perrottet dacian \n",
      "\n",
      "Topic 70: \n",
      "worker port union rail boral aurizon terminal work container agreement \n",
      "\n",
      "Topic 71: \n",
      "trading account fee exchange security report asx financial client broker \n",
      "\n",
      "Topic 72: \n",
      "copper dam oyu tolgoi river samarco olympic davy mongolian turquoise \n",
      "\n",
      "Topic 73: \n",
      "fund investment capital investor management manager asset equity amp private \n",
      "\n",
      "Topic 74: \n",
      "awe bruce waitsia bang buck midcap voluntarily audio decarbonising dwarfed \n",
      "\n",
      "Topic 75: \n",
      "gas lng energy woodside coast venture oil project east field \n",
      "\n",
      "Topic 76: \n",
      "tax wesfarmers british scott prime cameron white london johnson minister \n",
      "\n",
      "Topic 77: \n",
      "policy government political yesterday force german economic crisis reform security \n",
      "\n",
      "Topic 78: \n",
      "treasury clarke penfolds wolf grape wilkins blass margin britain region \n",
      "\n",
      "Topic 79: \n",
      "vitamin mcewan twitter article explanation bag guard carsales altium hero \n",
      "\n",
      "Topic 80: \n",
      "rebate cbh chester lam wpp inexperienced cbhs governs deeppocketed volunteered \n",
      "\n",
      "Topic 81: \n",
      "astrazeneca tripp vict unlicensed modernising magides nav formalise preclinical requisition \n",
      "\n",
      "Topic 82: \n",
      "blackmores swisse skincare underpayment holgate hony bortolussi gregory sheppard corbyn \n",
      "\n",
      "Topic 83: \n",
      "cleanaway bansal tote greenberg glamour endorsing blossom stud aventus drayton \n",
      "\n",
      "Topic 84: \n",
      "gold mining resource hydrogen mineral miner nickel production metal exploration \n",
      "\n",
      "Topic 85: \n",
      "stock investor share price index asx value valuation trading time \n",
      "\n",
      "Topic 86: \n",
      "china chinese trade global world country foreign beijing economy currency \n",
      "\n",
      "Topic 87: \n",
      "syrah canva redbubble rfg jdcom buchanan rfgs koutsantonis biomedical michels \n",
      "\n",
      "Topic 88: \n",
      "education university student training college stewart quality navitas vassella lucas \n",
      "\n",
      "Topic 89: \n",
      "trump european europe country brexit germany france trade italy union \n",
      "\n",
      "Topic 90: \n",
      "property estate blue real housing tabcorp sky wagering residential home \n",
      "\n",
      "Topic 91: \n",
      "wine brand content streaming video subscription racing netflix subscriber including \n",
      "\n",
      "Topic 92: \n",
      "virus steven health frozen awardwinning infected packet ethereum influencers patty \n",
      "\n",
      "Topic 93: \n",
      "sydney centre travel airport people service melbourne city group office \n",
      "\n",
      "Topic 94: \n",
      "seven sport right stokes network west league spark childcare broadcast \n",
      "\n",
      "Topic 95: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit share earnings growth billion dividend result analyst cost revenue \n",
      "\n",
      "Topic 96: \n",
      "gallagher royalty contract village mongolia outflow sand renewal paint inflow \n",
      "\n",
      "Topic 97: \n",
      "examining gorge deceptive misleading wam giles golf bookie walton charitable \n",
      "\n",
      "Topic 98: \n",
      "blackmore bunting tdm cowan initiating unanswered bookmaking honan companya gasmr \n",
      "\n",
      "Topic 99: \n",
      "executive chief board director chairman group ceo financial role management \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(lda_tf.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae1d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "financial commission report court regulator claim review action government case \n",
      "\n",
      "Topic 1: \n",
      "bank loan credit financial banking capital billion customer rate risk \n",
      "\n",
      "Topic 2: \n",
      "rate economy global china economic bond world central investor policy \n",
      "\n",
      "Topic 3: \n",
      "project group construction contract infrastructure road toll building billion contractor \n",
      "\n",
      "Topic 4: \n",
      "share shareholder board deal group investor offer capital executive director \n",
      "\n",
      "Topic 5: \n",
      "crown network medium telstra mobile casino nbn news service content \n",
      "\n",
      "Topic 6: \n",
      "share price growth profit earnings month billion stock analyst result \n",
      "\n",
      "Topic 7: \n",
      "say people time executive chief big like make think way \n",
      "\n",
      "Topic 8: \n",
      "energy power solar electricity vehicle car renewable battery generation wind \n",
      "\n",
      "Topic 9: \n",
      "coal port rail union queensland worker thermal aurizon terminal agreement \n",
      "\n",
      "Topic 10: \n",
      "sale store retailer brand retail woolworth online food customer product \n",
      "\n",
      "Topic 11: \n",
      "government project supply industry plant gas energy emission climate state \n",
      "\n",
      "Topic 12: \n",
      "technology product pandemic customer data service platform digital china tech \n",
      "\n",
      "Topic 13: \n",
      "trump president party political tax minister election leader state american \n",
      "\n",
      "Topic 14: \n",
      "fund investment manager investor management asset amp private industry insurance \n",
      "\n",
      "Topic 15: \n",
      "bhp mining copper resource project billion production lithium bhps miner \n",
      "\n",
      "Topic 16: \n",
      "gas oil price lng project energy santos billion production woodside \n",
      "\n",
      "Topic 17: \n",
      "iron ore rio tonne price steel fortescue china production miner \n",
      "\n",
      "Topic 18: \n",
      "gold hydrogen lynas point star ounce earth rare spot change \n",
      "\n",
      "Topic 19: \n",
      "airline travel airport virgin qantas international flight sydney hong kong \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(lda_tf_20.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:30]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd004668",
   "metadata": {},
   "source": [
    "### Calculating Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec32b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7a2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Cv(model, df_columnm):\n",
    "    topics = model.components_\n",
    "    n_top_words = 20\n",
    "    texts = [[word for word in doc.split()] for doc in df_columnm]\n",
    "    # create the dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Create a gensim dictionary from the word count matrix\n",
    "    # Create a gensim corpus from the word count matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "    # Get the top words for each topic from the components_ attribute\n",
    "    top_words = []\n",
    "    for topic in topics:\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    coherence_model = CoherenceModel(topics=top_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0ba4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Umass(model, df_columnm):\n",
    "    topics = model.components_\n",
    "    n_top_words = 20\n",
    "    texts = [[word for word in doc.split()] for doc in df_columnm]\n",
    "    # create the dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Create a gensim dictionary from the word count matrix\n",
    "    # Create a gensim corpus from the word count matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "    # Get the top words for each topic from the components_ attribute\n",
    "    top_words = []\n",
    "    for topic in topics:\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    coherence_model = CoherenceModel(topics=top_words, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f43735",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_cv = get_Cv(lda_tf_20,df['clean_text'])\n",
    "print(coherence_cv)\n",
    "coherence_umass = get_Umass(lda_tf_20,df['clean_text'])\n",
    "print(coherence_umass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "094c5268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5856644661189552\n",
      "-17.13975540062799\n"
     ]
    }
   ],
   "source": [
    "coherence_cv = get_Cv(lda_tf_50,df['clean_text'])\n",
    "print(coherence_cv)\n",
    "coherence_umass = get_Umass(lda_tf_50,df['clean_text'])\n",
    "print(coherence_umass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e97f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aac0b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5864202981780567\n",
      "-17.21704266897991\n"
     ]
    }
   ],
   "source": [
    "coherence_cv = get_Cv(lda_tf_100,df['clean_text'])\n",
    "print(coherence_cv)\n",
    "coherence_umass = get_Umass(lda_tf_100,df['clean_text'])\n",
    "print(coherence_umass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d404b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5905801352521686\n",
      "-17.22191333979769\n"
     ]
    }
   ],
   "source": [
    "coherence_cv = get_Cv(lda_tf_500,df['clean_text'])\n",
    "print(coherence_cv)\n",
    "coherence_umass = get_Umass(lda_tf_500,df['clean_text'])\n",
    "print(coherence_umass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6bc7dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
      "topic                                                \n",
      "0      0.209979 -0.181601       1        1  16.765831\n",
      "95     0.256229 -0.120966       2        1   7.717867\n",
      "49     0.267625 -0.104464       3        1   5.277460\n",
      "21     0.251136 -0.081986       4        1   4.672997\n",
      "99     0.218792 -0.077639       5        1   3.776451\n",
      "...         ...       ...     ...      ...        ...\n",
      "14    -0.184320 -0.117605      96        1   0.017992\n",
      "69    -0.192510 -0.148727      97        1   0.014117\n",
      "98    -0.191106 -0.142903      98        1   0.013660\n",
      "45    -0.193451 -0.152595      99        1   0.012376\n",
      "81    -0.194328 -0.157216     100        1   0.011732\n",
      "\n",
      "[100 rows x 5 columns], topic_info=             Term          Freq         Total  Category  logprob  loglift\n",
      "2193         bank  48591.000000  48591.000000   Default  30.0000  30.0000\n",
      "24783       share  43732.000000  43732.000000   Default  29.0000  29.0000\n",
      "11206         gas  22805.000000  22805.000000   Default  28.0000  28.0000\n",
      "11019        fund  30897.000000  30897.000000   Default  27.0000  27.0000\n",
      "20764       price  50980.000000  50980.000000   Default  26.0000  26.0000\n",
      "...           ...           ...           ...       ...      ...      ...\n",
      "21011  productsas      0.010486     11.435874  Topic100 -11.6532   2.0561\n",
      "16854  microphone      0.010486     24.975231  Topic100 -11.6532   1.2750\n",
      "13694      inplay      0.010486    116.331372  Topic100 -11.6532  -0.2636\n",
      "9670      explain      0.010486   1036.732372  Topic100 -11.6532  -2.4509\n",
      "21367  puntersthe      0.010486     10.128168  Topic100 -11.6532   2.1776\n",
      "\n",
      "[5089 rows x 6 columns], token_table=       Topic      Freq       Term\n",
      "term                             \n",
      "0          9  0.027733        aaa\n",
      "0         37  0.963732        aaa\n",
      "2         53  0.997369       aaco\n",
      "10        25  0.994375        aba\n",
      "21        50  0.991082  abatement\n",
      "...      ...       ...        ...\n",
      "30926     57  0.682827       zinc\n",
      "30927     79  0.998439        zip\n",
      "30928     64  0.985504     zircon\n",
      "30934     13  0.986715        zoo\n",
      "30935     47  0.997925       zoom\n",
      "\n",
      "[15446 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 96, 50, 22, 100, 48, 52, 74, 3, 64, 20, 63, 41, 72, 94, 62, 44, 60, 87, 56, 37, 38, 17, 86, 40, 76, 61, 23, 43, 78, 71, 51, 32, 58, 59, 11, 13, 91, 90, 92, 21, 33, 77, 14, 66, 85, 65, 27, 6, 68, 19, 28, 36, 67, 95, 25, 73, 55, 16, 31, 26, 29, 9, 97, 57, 34, 2, 80, 30, 89, 4, 54, 79, 8, 5, 18, 49, 12, 47, 83, 98, 93, 75, 35, 69, 39, 42, 24, 84, 45, 7, 53, 81, 10, 88, 15, 70, 99, 46, 82])\n",
      "Serving to http://127.0.0.1:8891/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Apr/2023 11:57:28] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Apr/2023 11:57:31] code 404, message Not Found\n",
      "127.0.0.1 - - [24/Apr/2023 11:57:31] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    " # #------------------------  pyLDAvis visualisation   -------------------------\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "data = pyLDAvis.lda_model.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "print(data)\n",
    "\n",
    "pyLDAvis.show(data, local = False)\n",
    "pyLDAvis.save_json(data,'fileobj.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a85ee1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_json(data,'fileobj.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff28bd3",
   "metadata": {},
   "source": [
    "### (Exploratory) Train LDA using octis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87daf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[:300]\n",
    "test = test.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('testdata/corpus.tsv',sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "os.chdir(os.path.pardir)\n",
    "\n",
    "# Initialize preprocessing\n",
    "preprocessor = Preprocessing(vocabulary=4, max_features=4,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list='english',\n",
    "                             min_chars=1, min_words_docs=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "dataset = preprocessor.preprocess_dataset(documents_path='testdata/corpus.tsv', labels_path='testdata/vocabulary.txt')\n",
    "# save the preprocessed dataset\n",
    "dataset.save('hello_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.dataset.dataset import Dataset\n",
    "from octis.models.LDA import LDA\n",
    "\n",
    "model = LDA(num_topics=25)  # Create model\n",
    "model_output = model.train_model(dataset) # Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b934ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "metric = TopicDiversity(topk=10) # Initialize metric\n",
    "topic_diversity_score = metric.score(model_output) # Compute score of the metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
